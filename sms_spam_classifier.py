# -*- coding: utf-8 -*-
"""SMS_spam_classifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1W57r7SdbiaN4pWPRucsSpda-_CORAhDK

### 1.Import Libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

"""### 2.Import Dataset from Kaggle"""

from google.colab import files
files.upload()

# create a kaggle directory
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/

# permission for json files to act
!chmod 600 ~/.kaggle/kaggle.json

# place the copied API command
!kaggle datasets download -d uciml/sms-spam-collection-dataset

# import the zip library
!pip install zip_files

# open the zip files of the dataset
from zipfile import ZipFile
# open the zipped file
file_name="sms-spam-collection-dataset.zip"
with ZipFile(file_name,'r') as zip:
      zip.extractall()
      print('Done')

df = pd.read_csv(r'/content/spam.csv', encoding='latin-1')

df.sample(5)

# check size of the dataframe
df.size

# check the shape
df.shape



"""Tasks:

1.Data cleaning

2.Exploratory Data Analysis(EDA)

3.Preprocessing

4.Model building

5.Model Evaluation

6.Model improvement

7.Create website

8.Deploy

### 3.Data Cleaning
"""

# get info about the dataframe
df.info()

# we do not need the unnamed columns, remove them
df.drop(columns=['Unnamed: 2','Unnamed: 3','Unnamed: 4'], inplace=True)

df.head()

# rename the columns of the dataframes
df.rename(columns={'v1':'target', 'v2':'text'}, inplace=True)
df.head()

# Since the target variable has 2 categories/binary outputs, we can encode them into [1,0]
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()

df['target'] = le.fit_transform(df['target'])

df.head()

# check for null values
df.isnull().sum()

# check for duplicates
df.duplicated().sum()

# drop the duplicated values
df = df.drop_duplicates(keep='first')

df.duplicated().sum()

df.shape

"""### 4.Exploratory Data Analysis(EDA)"""

df.head()

# lets check the value count of each class since we have two classes and see if there is probaly unbalanced data
df['target'].value_counts()

# lets plot a pie chart to properly see the classes/categories
plt.pie(df['target'].value_counts(), labels=['ham','spam'], autopct="%0.2f")
plt.show()

# obviously the data is imbalance

import re # this is for regular expressions when we are trying to manipulate the text
import nltk #library used for nlp products

nltk.download('stopwords') # download the stopwords( which are words that do not provide any meaning and value to the text being
# analyzed for insights, basically are words like 'the, are, is etc', not the NOT word because the word can tell us whether the 
# review is positive or negative)

nltk.download('punkt') # tokenizer divides a text into a list of sentences by using an unsupervised algorithm to build a model for abbreviation words,
# collocations, and words

# create a new column for num of characters for each sentence, get a lenght of characters for each sentence
df['num_of_characters'] = df['text'].apply(len)
df.head()

from nltk import word_tokenize, sent_tokenize # these are used to tokenize or seperate the sentences and words into tokens so they
# can be taken easily by the corpus

# now create a column for number of words in a sentence, this is not gonna be the same number as characters, obviously
df['num_of_words'] = df['text'].apply(lambda x:len(word_tokenize(x)))
df.head()

# create column to get number of sentences
df['num_of_sentences'] = df['text'].apply(lambda x:len(sent_tokenize(x)))
df.head()

df[['num_of_characters','num_of_words','num_of_sentences']].describe()

# for the class-0, describe its data
df[df['target']==0][['num_of_characters','num_of_words','num_of_sentences']].describe()

# for the class-1, describe its data
df[df['target']==1][['num_of_characters','num_of_words','num_of_sentences']].describe()

# lets try to plot the num of characters that are spam and not
plt.figure(figsize=(13,7))
sns.histplot(df[df['target']==0]['num_of_characters'])
sns.histplot(df[df['target']==1]['num_of_characters'], color='red')
plt.show()

# plot histogram for num of words that are spam and not
plt.figure(figsize=(13,7))
sns.histplot(df[df['target']==0]['num_of_words'])
sns.histplot(df[df['target']==1]['num_of_words'], color='red')
plt.show()

# pair plot
sns.pairplot(df, hue='target')
plt.show()

#  correlation matrix
sns.heatmap(df.corr(), annot=True)
plt.show()

"""### 5.Preprocessing"""

# lowercase
# Tokenization
# remove special characters
# remove stop words and punctuation
# apply porter stemming

from nltk.stem import PorterStemmer # this library helps woth stemming process of finding the root of different words and place
# them under one column root of that word, to avoid having maultiple redundant words
ps = PorterStemmer()
from nltk.corpus import stopwords # here we are importing the stopwords to be used for the texts

def transform_text(text):
  #lowercase
  text = text.lower()
  #tokenization
  text = word_tokenize(text)
  # remove special characters
  y=[] # list that will contain are alphanumeric(alphabetic and numerical)
  for i in text:
    if i.isalnum():
      y.append(i)

  text = y[:] # take everything from start to last one, place in in text
  y.clear() # remove alphanumeric

  # remove stop words and punctuation
  for i in text:
    if i not in stopwords.words('english') and i not in string.punctuation:
      y.append(i)

  text = y[:] 
  y.clear() 

  # apply porter stemming
  for i in text:
    y.append(ps.stem(i))

  return " ".join(y)

import string

transform_text("I'm gonna be home soon and i don't want to talk about this stuff anymore tonight, k? I've cried enough today.")

df['text'][10]

# apply the tranform function to the whole dataframe
df['transformed_text'] = df['text'].apply(transform_text)

df.head()

# create a word cloud to see the words that are mostly appearing in these sms's that are scam
from wordcloud import WordCloud
wc = WordCloud(width=500, min_font_size=10, background_color='white')

spam_wordcloud = wc.generate(df[df['target']==1]['transformed_text'].str.cat(sep=" "))

plt.figure(figsize=(15,7))
plt.imshow(spam_wordcloud)
plt.show()

# wordcloud for not scam sms words
normal_wordcloud = wc.generate(df[df['target']==0]['transformed_text'].str.cat(sep=" "))

plt.figure(figsize=(15,7))
plt.imshow(normal_wordcloud)
plt.show()

# create a corpus for spam sms and dispay the words
spam_corpus =[]
for msg in df[df['target']==1]['transformed_text'].tolist():
  for word in msg.split():
    spam_corpus.append(word)

len(spam_corpus)

from collections import Counter

#plot the corpus
sns.barplot(pd.DataFrame(Counter(spam_corpus).most_common(30))[0], pd.DataFrame(Counter(spam_corpus).most_common(30))[1])
plt.xticks(rotation='vertical')
plt.show()

# create a corpus for not spam sms and dispay the words
normal_corpus =[]
for msg in df[df['target']==0]['transformed_text'].tolist():
  for word in msg.split():
    normal_corpus.append(word)

len(normal_corpus)

#plot the corpus
sns.barplot(pd.DataFrame(Counter(normal_corpus).most_common(30))[0], pd.DataFrame(Counter(normal_corpus).most_common(30))[1])
plt.xticks(rotation='vertical')
plt.show()

df.head()

# Text vectorization
# bag of words



"""### 6.Model building"""

from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
cv = CountVectorizer()
tfid = TfidfVectorizer(max_features=3000)

X= tfid.fit_transform(df['transformed_text']).toarray()

X.shape

y=df['target'].values

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state = 0)

# import all the naive bayes libraries
from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score

# create the models
gnb = GaussianNB()
mnb = MultinomialNB()
bnb = BernoulliNB()

# fit the gaussian naive bayes
gnb.fit(X_train, y_train)
y_pred_gnb = gnb.predict(X_test)
print(accuracy_score(y_test, y_pred_gnb))
print(confusion_matrix(y_test, y_pred_gnb))
print(precision_score(y_test, y_pred_gnb))

# fit the multinomial naive bayes
mnb.fit(X_train, y_train)
y_pred_mnb = mnb.predict(X_test)
print(accuracy_score(y_test, y_pred_mnb))
print(confusion_matrix(y_test, y_pred_mnb))
print(precision_score(y_test, y_pred_mnb))

# fit the bernouli naive bayes
bnb.fit(X_train, y_train)
y_pred_bnb = mnb.predict(X_test)
print(accuracy_score(y_test, y_pred_bnb))
print(confusion_matrix(y_test, y_pred_bnb))
print(precision_score(y_test, y_pred_bnb))

# lets try other classification models and see
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.ensemble import GradientBoostingClassifier
from xgboost import XGBClassifier

# create the above models
mnb = MultinomialNB()
log = LogisticRegression(penalty='l2', solver= 'liblinear')
svc = SVC(kernel='rbf', gamma=1.0)
knn = KNeighborsClassifier()
dtc = DecisionTreeClassifier(max_depth=5)
rft = RandomForestClassifier(random_state=0, n_estimators=50)
abc = AdaBoostClassifier(random_state=0, n_estimators=50)
bc = BaggingClassifier(random_state=0,  n_estimators=50)
etc = ExtraTreesClassifier(random_state=0, n_estimators=50)
gbc = GradientBoostingClassifier(random_state=0, n_estimators=50)
xgb = XGBClassifier(random_state=0, n_estimators=50)

# fit all the models using the training data
def train_classifier_model(clf,X_train,y_train,X_test,y_test):
  clf.fit(X_train,y_train)
  y_pred = clf.predict(X_test)
  accuracy = accuracy_score(y_test,y_pred)
  #confusion_matrix(y_test, y_pred)
  precision = precision_score(y_test, y_pred)

  return accuracy, precision

clfs_dict = {
    'MNB': mnb,
    'LOG': log,
    'SVC': svc,
    'KNN': knn,
    'DTC': dtc,
    'RFT':rft,
    'AdaBC':abc,
    'BagC':bc,
    'ExtaTC':etc,
    'GradientBC':gbc,
    'XtremeGB':xgb
}

# list that will contain the scores
accuracy_list =[]
precision_list =[]

# fitting all the models
for name,clf in clfs_dict.items():
  cur_accuracy, cur_precision = train_classifier_model(clf,X_train,y_train,X_test, y_test)
  print("For : ", name)
  print("Accuracy = ",cur_accuracy)
  print("Precision = ",cur_precision)

  accuracy_list.append(cur_accuracy)
  precision_list.append(cur_precision)





"""### 7.Model Evaluation"""

# create a dataframe to  plot rge results
performance_df = pd.DataFrame({'Algorithm':clfs_dict.keys(),'Accuracy':accuracy_list,'Precision':precision_list}).sort_values('Precision',ascending=False)

performance_df

performance_df1 = pd.melt(performance_df, id_vars="Algorithm")

performance_df1

# plot the results
sns.catplot(x='Algorithm', y='value', hue ='variable', data=performance_df1, kind='bar', height=5)
plt.ylim(0.5,1.0)
plt.xticks(rotation='vertical')
plt.show()



"""### 8.Model Improvement"""

#  change the max features of TFidf
temp_df1 = pd.DataFrame({'Algorithm':clfs_dict.keys(),'Accuracy_max_ft_3000':accuracy_list,'Precision_max_ft_3000':precision_list}).sort_values('Precision_max_ft_3000',ascending=False)

temp_df2 = pd.DataFrame({'Algorithm':clfs_dict.keys(),'Accuracy_scaling':accuracy_list,'Precision_scaling':precision_list}).sort_values('Precision_scaling',ascending=False)

new_df = performance_df.merge(temp_df1, on='Algorithm')

new_scaled_df = new_df.merge(temp_df2, on='Algorithm')

new_scaled_df

temp_chars_df = pd.DataFrame({'Algorithm':clfs_dict.keys(),'Accuracy_num_chars':accuracy_list,'Precision_num_chars':precision_list}).sort_values('Precision_num_chars',ascending=False)

final_df = new_scaled_df.merge(temp_chars_df, on='Algorithm')

final_df

# voting classifier
svc = SVC(kernel='sigmoid',gamma=1.0, probability=True)
from sklearn.ensemble import VotingClassifier
voting_clf = VotingClassifier(estimators=[('svm',svc),('mnb',mnb),('etc',etc)], voting='soft')

voting_clf.fit(X_train,y_train)

y_pred_voting = voting_clf.predict(X_test)
print("Accuracy: ", accuracy_score(y_test,y_pred_voting))
print("Precision: ", precision_score(y_test,y_pred_voting))

# Apply stacking 
estimators =[('svm',svc),('mnb',mnb),('etc',etc)]
final_estimator = RandomForestClassifier()

from sklearn.ensemble import StackingClassifier

stacking_clf = StackingClassifier(estimators=estimators, final_estimator=final_estimator)

stacking_clf.fit(X_train,y_train)

y_pred_stacking = stacking_clf.predict(X_test)
print("Accuracy: ", accuracy_score(y_test,y_pred_stacking))
print("Precision: ", precision_score(y_test,y_pred_stacking))

# save the model
import pickle
from google.colab import drive
drive.mount('/content/drive')

model_file = open('/content/drive/MyDrive/SMS Spam Model Data/spam_clf_model.pkl','wb')
vectorizer_file = open('/content/drive/MyDrive/SMS Spam Model Data/vectorizer.pkl','wb')
pickle.dump(tfid, vectorizer_file)
pickle.dump(mnb,model_file)

"""### 9.Create Website

### 10.Deploy
"""







































